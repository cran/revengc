%\VignetteIndexEntry{R packages: LaTeX vignettes}
%\VignetteKeyword{R}
%\VignetteKeyword{package}
%\VignetteKeyword{vignette}
%\VignetteKeyword{LaTeX}
%\VignetteEngine{R.rsp::tex}


%\VignetteEncoding{UTF-8}



\documentclass{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage[labelformat=empty]{caption}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{url}
\usepackage[margin=1in]{geometry}

\date{\vspace{-5ex}}

\usepackage[utf8]{inputenc}




\title{revengc: An R package to reverse engineer summarized data}
\author{Samantha Duchscherer, Robert Stewart, and Marie Urban
\\
Oak Ridge National Laboratory, 1 Bethel Valley Road, Oak Ridge, TN 37831
}

\begin{document}

\maketitle

\begin{abstract}
Decoupled (e.g. separate averages) and censored (e.g. $>$ 100 species) variables are continually reported by many well-established organizations (e.g. World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), World Bank, and various national censuses).  The challenge therefore is to infer what the original data could have been given summarized information.  We present an R package that reverse engineers censored and/or decoupled count data with two main functions.  The \textit{cnbinom.pars()} function estimates the average and dispersion parameter of a censored univariate frequency table.  The \textit{rec()} function reverse engineers summarized data into an uncensored bivariate table of probabilities.
\end{abstract}


\section{Introduction}
The \textbf{revengc} R package was originally developed to help model building occupancy \cite{stewart}.  Household size and area of residential structures are typically found in any given national census. If a census revealed the raw data or provided a full uncensored contingency table (household size \textit{$*$} area), computing interior density as people per area would be straightforward. However, household size and area are often reported as decoupled variables (separate univariate frequency tables, average values, or a combination of the two).  Furthermore, if a contingency table is provided, it typically left ($<$, $\leq$), right ($>$, $\geq$, +), and interval (-) censored. This summarized information is problematic for numerous reasons.  How can a people per area ratio be calculated when no affiliation between the variables exist? If a census reports a household size average of 5.3, then how many houses are there with 1 person, 2 people,..., 10 people? If a census reports that there are 100 houses in an area of 26-50 square meters, then how many houses are in 26, 27,..., 50 square meters?  
\par
A tool that approximates negative binomial parameters from a censored univariate frequency table as well as estimates interior cells of a contingency table governed by negative binomial and/or Poisson marginals can also be useful for other areas ranging from demographic and epidemiological data to ecological inference problems.  For example, population and community ecologist could unpack censored organism counts (e.g. $<$ 20 species).  Modeling life expectancy and mortality, which are two variables that are notorious for being summarized in an average and/or censored frequency table form, could also benefit from \textbf{revengc}.  Other summarized examples include the average number of births, the number of new disease cases (censored table), the number of mutations in a gene (censored table) or average mutation rate, etc.  We attempt to accommodate for various application of count data by offering five scenarios that can be reverse engineered:

\begin{enumerate}
\item \textit{cnbinom.pars()} - An univariate frequency table estimates an average and dispersion parameter
\item \textit{rec()} - Decoupled averages estimates an uncensored contingency table of probabilities 
\item \textit{rec()} - Decoupled frequency tables estimates an uncensored contingency table of probabilities 
\item \textit{rec()} - An average and frequency table estimates an uncensored contingency table of probabilities 
\item \textit{rec()} - A censored contingency table estimates an uncensored contingency table of probabilities 
\end{enumerate}

\par

This paper proceeds with our reverse engineering methodology for the two main function: \textit{cnbinom.pars()} and \textit{rec()}.  We provide an in-depth analysis of how we implemented both the negative binomial and Poisson distribution as well as the \textbf{truncdist} and \textbf{mipfp} R package \cite{truncdist, mipfpciting}.  Since the \textbf{revengc} package has specific input requirements, we continue with an explanation of how to implement \textit{cnbinom.pars()} and \textit{rec()} with correctly formatted table(s).  We then provide coded examples that implements \textbf{revengc} on national census data (household size and area) and end with concluding remarks.  


\section{The Methodology: \textit{cnbinom.pars()}  }
The methodology for the \textit{cnbinom.pars()} function is relatively straightforward.  To estimate an average $\mu$ and dispersion \textit{r} parameter, a censored frequency table is fit to a negative binomial distribution using a maximum log-likelihood function customized to handle left ($<$, $\leq$), right ($>$, $\geq$, +), and interval (-) censored data.  To show an example, first recall the negative binomial distribution \( P(X = x| \mu, r) \) parameterized as a distribution of the number of failures \textit{X} before the \textit{r$^{th}$} success in independent trials (\ref{eq:1}).  With success probability \textit{p} in each trail, \textit{r} $\geq$ 0 and 0 $\leq$ \textit{p} $\leq$ 1 \cite{linden}.   

\begin{equation} \label{eq:1}
\begin{aligned}
P(X=x| r, p) ={x+r-1\choose x} p ^{r}(1-p) ^{y} &\equiv
P(X=x| r, \mu) {x+r-1\choose x} \left(  \frac{r}{\mu+r}\right)  ^{r}    \left( \frac{\mu}{\mu+r} \right) ^{y} \\
E(X) &= \frac{r(1-p)}{p} =  \mu  \\ 
V(X) &= \frac{r(1-p)}{p^2} = \mu + \frac{\mu^2}{r}
\end{aligned}
\end{equation}

\noindent
Now consider an arbitrary censored frequency table \textit{x} that has a combination of left censored (\textit{x $<$ c}), interval censored (\textit{a $\leq$ x $\leq$ b}), and right censored (\textit{x $>$ d}) data (i.e. \textit{a, b, c,} and \textit{d} represent the censoring limits).  The optimal $\mu$ and \textit{r} parameter for \textit{x} maximizes its custom log-likelihood function (\ref{eq:2}).        

\begin{equation}\label{eq:2}
\begin{aligned}
L\left(\mu, r|x\right)_{\log} = \\
&+ \sum_{x<c}\log\left(P\left(x<c|\mu, r\right)\right) \\
&+ \sum_{a \leq x \leq b}\log\left(P\left(a \leq x \leq b |\mu, r\right)\right) \\
&+ \sum_{x>d}\log\left(P\left(x>d|\mu, r\right)\right)
\end{aligned}
\end{equation}


\section{The Methodology: \textit{rec()}  }

\subsection{Overview}
\textit{rec()} is a statistical approach that estimates the probabilities of a 'true' contingency table given summarized information: two averages, two univariate frequency tables, a combination of an average and univariate frequency table, and a censored contingency table.  Figure \ref{figure:1} presents a methodology workflow.    

\subsection{Negative Binomial and Poisson Distribution}
When only an average is provided, we assume the average and variance are equal and rely on a Poisson distribution (i.e. the probability of observing \textit{x} events in a given interval is given by Equation \ref{eq:3}).  We understand that there are many cases where data has more variation than what is indicated by the Poisson distribution (i.e. overdispersion).  However, with limited data, the Poisson distribution is implemented due to its convenient property of having only one parameter, $\lambda$ = average.  For the cases with more data (i.e. univariate frequency table(s) or censored contingency table), we account for dispersion by relying on the more flexible negative binomial distribution (\ref{eq:1}).  Hence, in these cases, the \textit{cnbinom.pars()} function estimates the optimal average $\mu$ and dispersion \textit{r} parameters.  

\begin{figure}[htbp]
\begin{center}
  \makebox[\textwidth]{\includegraphics[width=1.2\textwidth]{workflow_overview}}
\end{center}
\caption{\textbf{Workflow of rec() function.}} 
\label{figure:1}
\end{figure}

\begin{equation}\label{eq:3}
\begin{aligned}
P\left(X = x\right) &= e^{-\lambda}\frac{\lambda^x}{x!} \\
E(X) &= \lambda \\
V(X) &= \lambda
\end{aligned}
\end{equation}

\subsection{truncdist R package}
With the negative binomial ($\mu$ and \textit{r}) and/or Poisson ($\lambda$) parameters, \textit{rec()} calculates truncated distributions to represent uncensored row (Xlowerbound:Xupperbound) and column (Ylowerbound:Yupperbound) margins.  Calculations use the \textbf{truncdist} R package \cite{truncdist}, and to provide a reference, Equation \ref{eq:4} gives the probability density function of a truncated X distribution over the interval \textit{(a,b]} (i.e. the negative binomial and/or Poisson probability density function is represented by \textit{g(}$\cdot$\textit{)} and their corresponding cumulative distribution function is denoted by \textit{G(}$\cdot$\textit{)}).  Note, truncated distributions are very practical in this context because the distributions (margins) are restricted to a desired row and column marginal length.    

\begin{equation}\label{eq:4}
 f_{X}(x)=\left\{
  \begin{array}{@{}ll@{}}
    \frac{g(x)}{G(b)-G(a)}, & \text{if}\ a < x \leq b \\
    0, & \text{otherwise}
  \end{array}\right.
\end{equation}

\par
The \textit{(a,b]} interval needed for both X (row of contingency table) and Y (column of contingency table) can be selected intuitively or with a brute force method.  If \textit{rec()} outputs a final contingency table with higher probabilities near the edge(s) of the table, then it would make sense to increase the range of the bound(s). For both variables, this would just involve making the lower bound less, making the upper bound more, or doing a combination of the two. The opposite holds true as well. If the final contingency table in \textit{rec()} has very low probabilities near the edge(s) of the table, the range of the particular bound(s) should be decreased.  

\subsection{mipfp R package}

\par
\textit{rec()} utilizing the \textbf{mipfp} R package to create cross tabulation probability estimates.  As mentioned above, the row and column marginals are the uncensored truncated distributions.  However, an opportunity for sensitivity analysis is presented by allowing an arbitrary seed estimation method and seed matrix.  Focusing on the seed estimation methods first, \textbf{mipfp} provides four algorithms (Table \ref{table:1}).  Essentially, all methods adjust cell proportions \( p_{xy} \) in a X $*$ Y contingency table to known marginal probabilities \( \pi_{x+} \) and \( \pi_{+y} \) (i.e. all interior cell estimates \( \hat{\pi}_{xy} \) are subject to marginal constraints (\ref{eq:5})).  For a better understanding, please refer the papers by \cite{little} and \cite{suesse}.     
 
\begin{equation}\label{eq:5}
\begin{aligned}
\sum_{y} \hat{\pi}_{x+} \qquad (x = 1, ..., X) \\
\sum_{x} \hat{\pi}_{+y} \qquad (y = 1, ..., Y)
\end{aligned}
\end{equation}
   

\begin{table}[htbp]
\bgroup
\def\arraystretch{1.4}
\begin{center}
\begin{tabular}{|c|c|c|}


\hline \textbf{Method} & \textbf{Abbreviation} & 
\begin{tabular}{@{}c@{}}  \textbf{Calculate \( \hat{\pi}_{xy} \) by} \end{tabular}  \\

\hline
\begin{tabular}{@{}c@{}}Iterative proportional \\ fitting procedure  \end{tabular} & ipfp &  
\begin{tabular}{@{}c@{}c@{}} Minimizing \\  \(  \sum_{x}\sum_{y}\hat{\pi}_{xy} ln (\hat{\pi}_{xy}/p_{xy}) \)    \end{tabular}  \\

\hline
Maximum likelihood method& ml  &   
\begin{tabular}{@{}c@{}}  Maximizing \\  \(  \sum_{x}\sum_{y}p_{xy}ln(\hat{\pi}_{xy})    \)  \end{tabular}  \\

\hline
Minimum chi-squared& chi2 & 
\begin{tabular}{@{}c@{}}  Minimizing \\  \(   \sum_{x}\sum_{j}(\hat{\pi}_{xy}-p_{xy})^2/\hat{\pi}_{xy}    \)  \end{tabular}  \\

\hline
Weighted least squares & lsq &  
\begin{tabular}{@{}c@{}}  Minimizing \\  \(   \sum_{x}\sum_{y} (p_{xy} - \hat{\pi}_{xy})^2/p_{xy}    \)  \end{tabular}  \\

\hline
\end{tabular}
\end{center}
\caption{\textbf{mipfp} methods to generate estimated cross tabulations \cite{mipfpciting}.}
\label{table:1}
\egroup
\end{table}

Now considering the arbitrary seed matrix, \textit{rec()} provides reasonable defaults.  For the decoupled cases (two averages, two tables, or a combination of a table and average), the absence of additional information makes it difficult to say much about the joint distribution.  Therefore, \textit{rec()} assumes independence between the variables, which is equivalent in making the X $*$ Y seed a matrix of ones (i.e. probability of this seed.matrix is 1 / sum(seed.matrix)).  When a censored contingency table is provided, independence does not have to be assumed and the interior cells can be weighted.  \textit{rec()} creates the default seed matrix by first repeating probability cells, which corresponding to the censored contingency table, for the newly created and compatible uncensored cross tabulations. Each cell value \textit{j} of this new seed.matrix is then changed to a probability \( seed.matrix[j]/sum(seed.matrix) \).  To see the seed for this case, refer to the Example section (Indonesia).    



\section{Usage}

\subsection{cnbinom.pars()}
The \textit{cnbinom.pars()} function has the following format with a description of the argument directly below.  The output is a list consisting of an estimated average (mu) and dispersion (r) parameter.  

\begin{verbatim}
cnbinom.pars(censoredtable)
\end{verbatim}

\noindent
\\
\textit{censoredtable}: A frequency table (censored and/or uncensored).  A data.frame and matrix are acceptable classes.  See \textbf{Data entry} section for formatting.
\\ 

\subsection{rec()}
The \textit{rec()} function has the following format with a description of each argument directly below.  The output is a list containing an uncensored contingency table of probabilities (rows range from Xlowerbound:Xupperbound and the columns range from Ylowerbound:Yupperbound) as well as the row and column parameters used in making the margins for the \textbf{mipfp} R package. 

\begin{verbatim}
rec(X, Y, Xlowerbound, Xupperbound, Ylowerbound, Yupperbound, 
  seed.matrix, seed.estimation.method)
\end{verbatim}
 
\noindent
\\
\textit{X}: Argument can be an average, a univariate frequency table, or a censored contingency table.  The average value should be a numeric class while a data.frame or matrix are acceptable table classes.  Y defaults to NULL if X argument is a censored contingency table.  See \textbf{Data entry} section for formatting.
\\ \\
\textit{Y}: Same description as X but this argument is for the Y variable.  X defaults to NULL if Y argument is a censored contingency table.
\\ \\
\textit{Xlowerbound}: A numeric class value to represent the left bound for X (row in contingency table).  The value must strictly be a non-negative integer and cannot be greater than the lowest category/average value provided for X (e.g. the lower bound cannot be 6 if a table has '$\geq$ 5' as a X or row category). 
\\ \\
\textit{Xupperbound}: A numeric class value to represent the right bound for X (row in contingency table).  The value must strictly be a non-negative integer and cannot be less than the highest category/average value provided for X (e.g. the upper bound cannot be 90 if a table has '$>$ 100' as a X or row category).
\\ \\
\textit{Ylowerbound}: Same description as Xlowerbound but this argument is for Y (column in contingency table). 
\\ \\
\textit{Yupperbound}: Same description as Xupperbound but this argument is for Y (column in contingency table). 
\\ \\
\textit{seed.matrix}: An intial probability matrix to be updated.  If decoupled variables is provided the default is a Xlowerbound:Xupperbound by Ylowerbound:Yupperbound seed.matrix with interior cells of 1 / sum(seed.matrix).  If a censored contingency table is provided the default is the \(seedmatrix()\$Probabilities\) output.
\\ \\
\textit{seed.estimation.method}: A character string indicating which method is used for updating the seed.matrix. The choices are: "ipfp", "ml", "chi2", or "lsq". Default is "ipfp".  

\section{Data entry}
The input tables are formatted to accommodate most open source data.  The univariate frequency table used in \textit{cnbinom.pars()} and/or \textit{rec()} needs to be a data.frame or matrix class with two columns and n rows.  The categories must be in the first column with the frequencies or probabilities in the second column.  Row names should never be placed in this table (the default row names should always be 1:n).  Column names can be any character string.  The only symbols accepted for censored data are listed below.  Note, less than or equal to ($\leq$ and LE) is not equivalent to less than ($<$ and L) and greater than or equal to ($\geq$, +, and GE) is not equivalent to greater than ($>$ and G).  Also, \textbf{revengc} uses closed intervals.  
\\ \\
\begin{itemize}
\item \textbf{Left censored symbols:} $<$, $\leq$, L, and LE\\
\item \textbf{Interval censored symbols:} $-$ and I (symbol has to be placed in the middle of the two category values)\\
\item \textbf{Right censored symbols:} $>$, $\geq$, +, G, and GE\\
\item \textbf{Uncensored symbol:} no symbol (only provide category value)
\end{itemize}
\par

To provide examples, the three tables below use different censored symbols yet give the same \textit{cnbinom.pars()} output ~\ref{table:2}. 

\begin{table}[htbp]
\begin{center}
\begin{tabular}{lclclc|}
\begin{tabular}[t]{|p{1.5 cm}|c|} 
\hline
Category & Frequency \\
\hline
$\leq$ 6 & 11800 \\
\hline
7-12& 57100 \\
\hline
13-19& 14800 \\
\hline
20+& 3900 \\
\hline
\end{tabular}



\begin{tabular}[t]{|p{1.5 cm}|c|}
\hline
Category & Frequency \\
\hline
LE 6 & 11800 \\
\hline
7 I 12& 57100 \\
\hline
13 I 19& 14800 \\
\hline
GE 20& 3900 \\
\hline
\end{tabular}


\begin{tabular}[t]{|p{1.5 cm}|c|}
\hline
Category & Frequency \\
\hline
$<$ 7 & 11800 \\
\hline
7 I 12& 57100 \\
\hline
13-19& 14800 \\
\hline
$\geq$ 20& 3900 \\
\hline
\end{tabular}
\end{tabular}
\end{center}
\caption{Examples of correctly formatted univariate tables.}
\label{table:2}
\end{table}

\par
The censored contingency table for \textit{rec()} has a similar format.  The censored symbols should follow the requirements listed above.  The table's class can be a data.frame or a matrix.  The column names should be the Y category values. The first column should be the X category values and the row names can be arbitrary.  The inside of the table are X $*$ Y frequencies, which are either non-negative frequencies or probabilities if seed.estimation.method is "ipfp" or strictly positive when method is "ml", "lsq" or "chi2". The row and column marginal totals corresponding to their X and Y category values need to be placed in this table. The top left, top right, and bottom left corners of the table should be NA or blank.  The bottom right corner can be a total cross tabulation sum value, NA, or blank. Table ~\ref{table:3} is a formatted example.  

\begin{table}[htbp]
\begin{center}
\begin{tabular}{| c | c | c | c | c |} 
\hline
NA & $<$20 & 20-30 & $>$30& NA\\
\hline
$<$5 & 18 & 19 & 8 & 45\\
\hline
5-9 & 13 & 8 & 12 & 33\\
\hline
$\geq$10 & 7 & 5 & 10 & 21\\
\hline
NA & 38 & 32 & 31 & NA\\
\hline
\end{tabular}
\end{center}
\caption{Example of a correctly formatted bivariate table.}
\label{table:3}
\end{table}


\subsection{Formatting tables in R}
The code below shows how to format these tables properly in R. 

\begin{verbatim}
# read in csv file
# univariatetable.csv is a preloaded example
# univariatetable.csv<-read.csv("filename.csv", row.names = NULL, 
#  header= FALSE, check.names=FALSE)
  
# create univariate table  
univariatetable<-cbind(as.character(c("1-2", "3-4", "5-6", "7-8", ">=9")), 
  c(16.2, 41.7, 29.0, 9.0, 4.1))
 
# create contingency table
# contingencytable.csv is a preloaded example that provides the same table
contingencytable<-matrix(c(6185,9797,16809,11126,6156,3637,908,147,69,4,
  5408,12748,26506,21486,14018,9165,2658,567,196,78,
  7403,20444,44370,36285,23576,15750,4715,994,364,136,
  4793,17376,44065,40751,28900,20404,6557,1296,555,228,
  2354,11143,32837,33910,26203,19301,6835,1438,618,245,
  1060,6038,19256,21298,17774,13864,4656,1039,430,178,
  273,2521,9110,11188,9626,7433,2608,578,196,112,
  119,1130,4183,5566,5053,3938,1367,318,119,66,
  33,388,1707,2367,2328,1972,719,171,68,37,
  38,178,1047,1672,1740,1666,757,193,158,164),
  nrow=10,ncol=10, byrow=TRUE)
rowmarginal<-apply(contingencytable,1,sum)
contingencytable<-cbind(contingencytable, rowmarginal)
colmarginal<-apply(contingencytable,2,sum)
contingencytable<-rbind(contingencytable, colmarginal)
row.names(contingencytable)[row.names(contingencytable)=="colmarginal"]<-""
contingencytable<-data.frame(c("1","2","3","4","5","6", "7", "8","9","10+", NA), 
  contingencytable)
colnames(contingencytable)<-c(NA,"<20","20-29","30-39","40-49","50-69","70-99",
  "100-149","150-199","200-299","300+", NA)
\end{verbatim}


\section{Worked examples}

\subsection {Nepal}
A Nepal Living Standards Survey \cite{nepal} provides both a censored table and average for urban household size.  We use the censored table to show that the \textit{cnbinom.pars()} function calculates a close approximation to the provided average household size (4.4 people).  Note, there is overdispersion in the data.   

\begin{verbatim}
# revengc has the Nepal household table preloaded as univariatetable.csv   
cnbinom.pars(censoredtable = univariatetable.csv)
\end{verbatim}


\subsection{Indonesia}
In 2010, the Population Census Data - Statistics Indonesia provided over 60 censored contingency tables containing Floor Area of Dwelling Unit (square meter) by Household Member Size. The tables are separated by province, urban, and rural.  Here we use the household size by area contingency table for Indonesia's rural Aceh Province to show the multiple coding steps and functions implemented inside \textit{rec()}.  This allows the user to see a methodology workflow in code form.  The final uncensored household size by area estimated probability table, which implemented the `ipfp` seed estimation method and default seed matrix, has rows ranging from 1 (Xlowerbound) to 15 (Xupperbound) people and columns ranging from 10 (Ylowerbound) to 310 (Yupperbound) square meters.   

\begin{verbatim}
# data = Indonesia 's rural Aceh Province censored contingency table
# preloaded as 'contingencytable.csv'
contingencytable.csv 

# provided upper and lower bound values for table
# X=row and Y=column
Xlowerbound=1
Xupperbound=15
Ylowerbound=10
Yupperbound=310

# table of row marginals provides average and dispersion for x
row.marginal.table<-row.marginal(contingencytable.csv)
x<-cnbinom.pars(row.marginal.table)
# table of column marginals provides average and dispersion for y
column.marginal.table<-column.marginal(contingencytable.csv)
y<-cnbinom.pars(column.marginal.table)

# create uncensored row and column ranges   
rowrange<-Xlowerbound:Xupperbound
colrange<-Ylowerbound:Yupperbound

# new uncensored row marginal table = truncated negative binomial distribution
uncensored.row.margin<-dtrunc(rowrange, mu=x$Average, size = x$Dispersion, 
  a = Xlowerbound-1, b = Xupperbound, spec = "nbinom")
# new uncensored column margin table = truncated negative binomial distribution
uncensored.column.margin<-dtrunc(colrange, mu=y$Average, size = y$Dispersion,
  a = Ylowerbound-1, b = Yupperbound, spec = "nbinom")

# sum of truncated distributions equal 1
# margins need to be equal for mipfp 
sum(uncensored.row.margin)
sum(uncensored.column.margin)

# create seed of probabilities (rec() default)
seed.output<-seedmatrix(contingencytable.csv, Xlowerbound, 
  Xupperbound, Ylowerbound, Yupperbound)$Probabilities

# run mipfp
# store the new margins in a list
tgt.data<-list(uncensored.row.margin, uncensored.column.margin)
# list of dimensions of each marginal constrain
tgt.list<-list(1,2)
# calling the estimated function
## seed has to be in array format for mipfp package
## ipfp is the selected seed.estimation.method
final1<-Estimate(array(seed.output,dim=c(length(Xlowerbound:Xupperbound), 
  length(Ylowerbound:Yupperbound))), tgt.list, tgt.data, method="ipfp")$x.hat

# filling in names of updated seed  
final1<-data.frame(final1)
row.names(final1)<-Xlowerbound:Xupperbound
names(final1)<-Ylowerbound:Yupperbound

# reweight estimates to known censored interior cells 
final1<-reweight.contingencytable(observed.table = contingencytable.csv, 
  estimated.table = final1)

# final results are probabilities
sum(final1)

# rec() function outputs the same table
# default of rec() seed.estimation.method is ipfp
# default of rec() seed.matrix is the output of seedmatrix()$Probabilities 
final2<-rec(X= contingencytable.csv,
  Xlowerbound = 1,
  Xupperbound = 15,
  Ylowerbound = 10,
  Yupperbound = 310)

# check that both data.frame results have same values
all(final1 == final2$Probability.Estimates)
\end{verbatim}

\section{Conclusion}
\textbf{revengc} was designed to reverse engineer summarized and decoupled variables with two main functions: \textit{cnbinom.pars()} and \textit{rec()}.  Relying on a negative binomial distribution, \textit{cnbinom.pars()} approximates the average and dispersion parameter of a censored univariate frequency table.  \textit{rec()} fills in missing interior cell values from observed aggregated data (i.e. decouples average(s) and/or censored frequency table(s) or a censored contingency table).  There are some required assumptions in \textit{rec()}.  For instance, \textit{rec()} relies on a Poisson distribution when only an average is provided, which is assuming the variance and average are equal.  More descriptive input variables, such as univariate tables or contingency tables, can account for dispersion found in data.  However, independence between decoupled variables still has to be assumed when there is no external information about the joint distribution.  For these reasons, \textbf{rec()} provides two options for sensitivity analysis: the seed matrix and the method used in updating the seed matrix are both arbitrary inputs.

\newpage

\bibliographystyle{ieeetr}
\bibliography{duchscherer-stewart-urban}

\end{document}